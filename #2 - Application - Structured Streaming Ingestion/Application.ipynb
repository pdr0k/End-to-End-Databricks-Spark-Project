{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2177acc6-2357-4569-8807-cf213e0e3412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting Faker\n  Using cached Faker-33.3.1-py3-none-any.whl (1.9 MB)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from Faker) (4.4.0)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.10/site-packages (from Faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.4->Faker) (1.16.0)\nInstalling collected packages: Faker\nSuccessfully installed Faker-33.3.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76470c23-2ef4-4afb-836f-4cf90826158f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1379593343009408>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#IMPORT LIBRARIES\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfaker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Faker \u001B[38;5;66;03m#generate fake data\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m \u001B[38;5;66;03m#generate pseudo-random numbers\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdbutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DBUtils \u001B[38;5;66;03m#work with files and object storage efficiently\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'faker'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ModuleNotFoundError",
        "evalue": "No module named 'faker'"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'faker'"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
        "File \u001B[0;32m<command-1379593343009408>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#IMPORT LIBRARIES\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfaker\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Faker \u001B[38;5;66;03m#generate fake data\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m \u001B[38;5;66;03m#generate pseudo-random numbers\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdbutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DBUtils \u001B[38;5;66;03m#work with files and object storage efficiently\u001B[39;00m\n",
        "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'faker'"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#IMPORT LIBRARIES\n",
    "from faker import Faker #generate fake data\n",
    "import random #generate pseudo-random numbers\n",
    "from pyspark.dbutils import DBUtils #work with files and object storage efficiently\n",
    "from faker.providers import internet # generate fake data\n",
    "from datetime import datetime #manipulating dates and times\n",
    "import pandas as pd #data manipulation\n",
    "import os #os interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf119ed-a647-4d55-9f12-a0cb9d36cc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">/**<br /> * Copies a file or directory, possibly across FileSystems..<br /> * <br /> * Example: cp(\"/mnt/my-folder/a\", \"s3n://bucket/b\")<br /> * <br /> * @param from FileSystem URI of the source file or directory<br /> * @param to FileSystem URI of the destination file or directory<br /> * @param recurse if true, all files and directories will be recursively copied<br /> * @return true if all files were successfully copied<br /> */<br /><b>cp(from: java.lang.String, to: java.lang.String, recurse: boolean = false): boolean</b></div><br />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help(\"cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9376ad22-2fcf-4297-a035-6415459295ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_rename_parquet(location, new_location):\n",
    "  \"\"\"\n",
    "  Renames and moves parquet file to a new location, then removes the original directory.\n",
    "\n",
    "  Parameters:\n",
    "  location (str): the present file location.\n",
    "  new_location (str): the file's new location.\n",
    "\n",
    "  Returns:\n",
    "  A print statement of the file's new location.\n",
    "  \"\"\"\n",
    "  from datetime import datetime\n",
    "\n",
    "  for file_info in dbutils.fs.ls(location):\n",
    "    #return of dbutils.fs.ls: [FileInfo(path='dbfs:/mnt/data/original_folder/data_2024-11-15.parquet', name='data_2024-11-15.parquet', size=1024)]\n",
    "    \n",
    "    file_path = file_info.path #getting the path\n",
    "\n",
    "    if file_path[-8:] == '.parquet':\n",
    "      #If file is a .parquet, move the file to the new location and remove the previous.\n",
    "      dbutils.fs.cp(file_path, new_location)\n",
    "      dbutils.fs.rm(location, True)\n",
    "\n",
    "  return print(f'File generated at: {new_location}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2b094ff-c702-4e03-a65c-71ba281f5226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\">/**<br /> * Removes a file or directory.<br /> * <br /> * Example: rm(\"/mnt/my-folder/\", true)<br /> * <br /> * @param dir FileSystem URI for a single file or a directory<br /> * @param recurse if true, all files and directories will be recursively deleted<br /> * @return true if the file or directory was present and is now deleted<br /> */<br /><b>rm(dir: java.lang.String, recurse: boolean = false): boolean</b></div><br />"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help(\"rm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d8e2d8-6164-4bd9-9943-05fccc3d54fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#generating data\n",
    "\n",
    "fake = Faker()\n",
    "fake.add_provider(internet)\n",
    "\n",
    "def gen_data(connections_quantity, device, status):\n",
    "  '''\n",
    "  Creates fake data of devices connections.\n",
    "\n",
    "  Parameters:\n",
    "  connections_quantity (int): the quantity of devices connections on the network.\n",
    "  device (str): the device that it'll be connecting.\n",
    "  status (str): the connections's status.\n",
    "  '''\n",
    "  \n",
    "  fake_records = []\n",
    "  speed_connection = [1, 5, 10, 15, 25, 35, 50, 100, 200, 400, 500, 1000]\n",
    "\n",
    "  #generating fake data list\n",
    "  for i in range(connections_quantity):\n",
    "    new_register = {\n",
    "      'Name': fake.name(),\n",
    "      'Address': fake.address(),\n",
    "      'IP': fake.ipv4_private(),\n",
    "      'Connection_Time': datetime.now(),\n",
    "      'Device': device,\n",
    "      'Speed_Connection': random.choice(speed_connection),\n",
    "      'Connection_Status': status\n",
    "    }\n",
    "    fake_records.append(new_register)\n",
    "\n",
    "  # 'from_records' method creates a dataframe from tuples, dicts\n",
    "  df = pd.DataFrame.from_records(fake_records)  \n",
    "  # converting pandas df to spark df\n",
    "  spark_df = spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "  #saving the data in the landing zone:\n",
    "  try:\n",
    "    #path of the generated file\n",
    "    local = f\"dbfs:/FileStore/landing_zone/{device}/{status}/{status}_{datetime.now()}\"\n",
    "    #write file\n",
    "    spark_df.coalesce(1).write.format(\"parquet\").mode(\"overwrite\").save(local)\n",
    "    new_local = f\"dbfs:/FileStore/landing_zone/{device}/{status}/{device}/{device}_{datetime.now()}.parquet\"\n",
    "\n",
    "    replace_rename_parquet(local, new_local)\n",
    "\n",
    "    return print(f'Sucessfully generated: {connections_quantity} connections of {device} | Status: {status} | Path: {new_local}')\n",
    "  \n",
    "  except:\n",
    "    return print(\"Error at gen_data().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da03f7f-cdea-40fa-8a0e-7b90a6f3c784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingestion_gen_data(argument):\n",
    "  '''\n",
    "  Generates the exact data we want for the ingestion.\n",
    "  \n",
    "  Parameters:\n",
    "  argument (str): key of the data we want to ingest.\n",
    "\n",
    "  Example:\n",
    "  ingestion_gen_data('Computer_Online')\n",
    "  '''\n",
    "\n",
    "  switch_case = {\n",
    "    'Computer_Online': lambda: gen_data(40, 'Computer', 'Online'),\n",
    "    'Mobile_Online': lambda: gen_data(80, 'Mobile', 'Online'),\n",
    "    'Computer_Offline': lambda: gen_data(70, 'Computer', 'Offline'),\n",
    "    'Mobile_Offline': lambda: gen_data(60, 'Mobile', 'Offline')\n",
    "  }\n",
    "  \n",
    "  #executes an item of switch_case\n",
    "  switch_case.get(argument, lambda: print('Not found!'))()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab22932f-b1ec-46b7-a2db-a60f1eb3b329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def delete_directory(dbutils, directory_path):\n",
    "    if dbutils.fs.mkdirs(f\"{directory_path}\"):\n",
    "        files = dbutils.fs.ls(f\"{directory_path}\")\n",
    "        if files:\n",
    "            for file in files:\n",
    "                if file.isDir():\n",
    "                    delete_directory(dbutils, file.path)\n",
    "                    print(f\"Files in the directory {file} excluded with success.\")\n",
    "                else:\n",
    "                    dbutils.fs.rm(file.path)\n",
    "                    print(f\"Files in the directory {file} excluded with success \")\n",
    "                    \n",
    "        dbutils.fs.rm(directory_path)\n",
    "\n",
    "        return print(f\"Files in the directory {directory_path} excluded with success\")\n",
    "    else:\n",
    "        return print(f\"The directory {directory_path} doesn`t exists.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Application",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
